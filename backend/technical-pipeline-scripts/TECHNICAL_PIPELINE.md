# MCP RAG AI Agent For Video Q&A

## High-Level Overview & Description

This AI agent answers user queries about videos that they see on a user interface like a TikTok For You feed featuring a scrollable feed of vertical short-form videos. The core functionality allows users to ask natural language questions about the video content—ranging from background details to character actions or timestamp-specific events—and receive informative answers generated by the AI agent.

**Core Architecture: Agentic AI via RAG and MCP**

This application exemplifies an **"Agentic AI flow."** Unlike simpler AI models that might generate responses based solely on internal training data or direct prompting, this system employs an AI agent that actively interacts with its environment (specifically, a curated knowledge base derived from the video) to ground its answers in **verifiable information extracted directly from the video content**. This is achieved through a **Retrieval-Augmented Generation (RAG)** pipeline integrated with a Model Context Protocol where our MCP Client uses tools from Perplexity's **Model Context Protocol (MCP) Server**, leveraging Perplexity's advanced internet search, reasoning, and research skills to empower the agent to respond to the user query with high quality, contextually accurate, and expertly researched answers.

## Intelligent Video Understanding & Response Workflow

Our AI agent processes and responds to queries through a sophisticated multi-stage workflow:

1. **Video Processing & Knowledge Base Construction:**

   - The system breaks down videos into meaningful chunks based on scene detection or fixed intervals
   - Each chunk is analyzed by multimodal AI to generate detailed captions describing visual elements, actions, audio, and sentiment
   - These captions, along with an overall video summary and key themes, are converted to vector embeddings and stored in Pinecone
   - The system tracks processing status throughout, updating from "PROCESSING" to "FINISHED" when indexing is complete

2. **Contextual Query Understanding & Retrieval:**

   - When a user asks a question about the video, their query is transformed into the same vector space as the video knowledge base
   - The system intelligently retrieves the most semantically relevant video segments by comparing query vectors to caption vectors
   - Retrieved content is assembled into a comprehensive context that includes the video summary, key themes, and specific relevant clips with their temporal information

3. **Knowledge Augmentation via Perplexity MCP:**

   - The enriched query context flows to our MCP client, which connects to Perplexity's Model Context Protocol servers
   - The system selects the most appropriate tool (perplexity_ask, perplexity_reason, or perplexity_research) based on query analysis
   - Perplexity MCP enhances the context with relevant external knowledge, ensuring responses incorporate both video-specific details and broader contextual information

4. **Response Synthesis & Delivery:**
   - The final answer is expertly crafted by synthesizing the original query, video-specific context, and Perplexity-sourced information
   - This creates responses that are not only accurate to the video content but enriched with relevant external knowledge when beneficial

This agentic approach enables our system to act with purpose and intelligence, actively seeking and integrating multiple knowledge sources rather than passively generating responses from static training data. The result is an AI assistant that delivers contextual, accurate, and insightful answers about video content.

## Technical Pipeline Breakdown

Here's a more detailed breakdown of the implemented technical steps that support this agentic flow:

1.  **Video Discovery & Acquisition:**

    - **(Discovery - Optional):** The `scrape_tiktok_url.py` script demonstrates a method for discovering relevant video URLs, acting as a potential input source for the system. It uses the `TikTokApi` library (requiring an `ms_token` from browser cookies for authentication) to search for trending videos. It applies filters based on criteria like maximum duration and minimum view count. It includes retry logic and can return a randomly selected matching URL or fall back to the shortest video found.
    - **(Acquisition):** Once a URL is obtained, the `download_from_url.py` script uses the `yt-dlp` library to download the video. It extracts identifiers (like TikTok username and TikTok Video ID) from the URL to create a dedicated directory (e.g., `videos/username-tiktokvideoid/`) and saves the downloaded video file (e.g., `username-tiktokvideoid.mp4`) within it. It also creates an initial JSON metadata file with a `processing_status` field set to "PROCESSING" to track the video's progression through the pipeline stages.

2.  **Video Chunking (Preparing the Raw Data):**

    - The downloaded video is segmented into smaller, manageable chunks using `chunk_by_scenes.py`.
    - **Scene Detection & Fallback:** The script utilizes the `PySceneDetect` library (specifically, the `ContentDetector`) to attempt detecting natural scene boundaries based on changes in content.
    - If it detects very few or no distinct scene changes (less than or equal to 1), indicating a static scene or detection limitations, it automatically falls back to splitting the video into **fixed-duration chunks** (e.g., 4 seconds each) to ensure the entire video is processed.
    - **Chunk Saving:** Each resulting video segment (whether from scene detection or fixed splitting) is saved as a separate `.mp4` file (e.g., `<video_id>-Scene-001.mp4`) within a `chunks` subdirectory (e.g., `videos/<video_id>/chunks/`). It leverages NVENC hardware acceleration for faster chunk creation if available.
    - **Metadata Generation:** The script calculates and stores detailed metadata that includes temporal information including:
      Top Level:
      - `video_id`: id of video in the format <TIKTOK_USER_NAME>-<TIKTOK_VIDEO_ID>
      - `num_chunks`: total number of video chunks
      - `total_duration_seconds`: Total video duration at the top level of the JSON
      - `chunks`: array of `chunk` JSON objects (see below for full details)
        Each `chunk` JSON object in the `chunks` array contains:
        - `chunk_name`: name of chunk file (e.g. `<video_id>-Scene-001.mp4`)
        - `start_timestamp` and `end_timestamp`: Formatted as "MM:SS.fff" for human readability
        - `chunk_number`: The sequential position of the chunk in the video (1-based index)
        - `chunk_duration_seconds`: The precise duration of each chunk in seconds
        - `normalized_start_time` and `normalized_end_time`: Values between 0.0 and 1.0 representing the relative position within the video
    - This comprehensive **JSON metadata file** (e.g., `videos/<video_id>/<video_id>.json`) serves as the manifest for all subsequent processing steps.

3.  **Caption Generation & Video Summarization (Building the Agent's Knowledge Base - Part 1):**

    - The `caption_chunks_and_summarize.py` script reads the JSON metadata file generated in the previous step.
    - **Caption Generation:** Each video chunk listed in the JSON is processed individually to generate a detailed textual description.
      - A Gemini multimodal model (`gemini-2.5-pro-exp-03-25`) analyzes the video chunk by first uploading the video chunk then processing it with Gemini's vision capabilities
      - Gemini generates a descriptive, accessible caption covering scene details, characters, actions, audio cues, and sentiment, specifically formatted for RAG suitability
      - The generated caption text is added back into the corresponding chunk's entry within the JSON metadata file
    - **Video Summarization:** After all chunks have been captioned, the script performs an additional step:
      - It concatenates all generated captions in chronological order (using the `chunk_number` to ensure proper sequence)
      - It concurrently calls OpenAI's API to perform two tasks:
        - Generate a comprehensive **overall summary** of the entire video content
        - Extract a list of **key themes/topics** present in the video
      - These insights are stored in the JSON file's top level as `overall_summary` and `key_themes`
      - A timestamp of when the summary was generated is also recorded and stored in the JSON file top level as `summary_generated_at`
    - This enriched JSON file now contains both granular scene-level descriptions and a holistic understanding of the video content, providing a multi-level knowledge base for the retrieval system.

4.  **Vector Embedding & Indexing (Building the Agent's Knowledge Base - Part 2):**

    - The `index_and_retrieve.py` script processes the JSON metadata file containing the generated captions.
    - **Duplicate Prevention:** Before generating embeddings, it efficiently checks the Pinecone index to identify which `chunk_name`s (representing video chunks) already exist. This prevents re-processing and ensures only _new_ chunks are added.
    - **Embedding:** For new chunks, it extracts the caption text. It uses the OpenAI API (`text-embedding-ada-002` model by default) via concurrent requests to generate dense vector embeddings for each caption. Concurrency significantly speeds up this potentially time-consuming step.
    - **Data Structuring:** Each caption's embedding is packaged into a vector object containing:
      - A unique `id` (the `chunk_name`).
      - The `values` (the embedding vector itself).
      - `metadata`: Crucial information like the original caption text, start/end timestamps, chunk name, and the source video name, allowing retrieval of context alongside the vector.
    - **Indexing:** These vector objects are then efficiently uploaded (upserted) into the designated Pinecone serverless index (`video-captions-index`) in batches. Batching improves indexing performance and reduces API calls. The script also includes logic to poll the index stats to help ensure vectors are queryable before potentially moving to the next step.
    - **Processing Status Update:** After successful indexing, the script updates the `processing_status` field in the video's JSON metadata file from "PROCESSING" to "FINISHED", indicating that the video has completed all pipeline stages and is ready for querying.

5.  **Retrieval (Seeking Relevant Information from vector database):**

    - When a user query is received (demonstrated with a sample question in `index_and_retrieve.py`), the retrieval process begins.
    - **Query Embedding:** The natural language query is converted into a vector embedding using the _same_ OpenAI embedding model (`text-embedding-ada-002`) that was used to embed the captions. This ensures the query vector resides in the same semantic space as the indexed caption vectors.
    - **Similarity Search & Filtering:** The script queries the Pinecone index using the generated query vector. It requests the `top_k` (e.g., top 3) most similar caption vectors based on cosine similarity (or the metric defined for the index). Crucially, it **filters the search to only include chunks belonging to the specific `video_id`** (extracted from the video's JSON metadata) being queried. This ensures relevance to the current video context.
    - **Context Retrieval:** Crucially, the query specifies `include_metadata=True`. This ensures that Pinecone returns not only the relevant vector IDs and their similarity scores but also the associated metadata (the actual caption text, timestamps, chunk name) that was stored during indexing. This retrieved metadata provides the necessary context for the next stage of answer generation.

6.  **Retrieval Augmentation & Context Assembly (`index_and_retrieve.py` -> `intermediate_prompt.txt`):**

    - After retrieving the relevant video chunk data from Pinecone, the `query_and_get_context` function constructs the final context needed for the downstream AI model.
    - **Information Gathering:** It first loads the `overall_summary`, `key_themes`, and `total_duration_seconds` from the video's JSON metadata file.
    - **Context Assembly:** It then assembles a structured text block which includes:
      - The **Video Summary**.
      - The identified **Key Themes**.
      - The **Total Video Duration**.
      - A list titled "Potentially Relevant Video Clips (in order)". Each retrieved clip in this list includes:
        - Its **sequence number** (e.g., "Clip 1/10").
        - Its **start and end timestamps**.
        - A **relative time hint** (e.g., "near the beginning", "around the middle", "near the end") based on its normalized start/end times.
        - The full **caption text** generated for that chunk.
    - **Output:** This assembled context, combined with the original **User Query**, forms the input that would typically be sent to the MCP workflow, where our MCP client would connect to Perplexity's MCP server and choose the best tool (perplexity_ask, perplexity_reason, or perplexity_research) to provide further useful information grounded in internet search results that may be beneficial to answer the user query. In the current demonstration (`index_and_retrieve.py`), this assembled context is simply printed and effectively represents the content shown in `intermediate_prompt.txt` (which was created manually for illustrative purposes in this version). This retrieval-augmented context provides the LLM with both a high-level understanding (summary, themes) and specific, relevant details (chunk captions) grounded in the video content, enabling a more informed and accurate response.

7.  **Answering (Agent Action: MCP Tool call, then Synthesizing & Generating Response - `mcp_client.py`):**

    This final step involves using the `mcp_client.py` script to interact with Perplexity's capabilities via the Model Context Protocol (MCP) and generate the final answer.

    - **MCP Connection & Input:** The `mcp_client.py` script is executed, typically connecting to a specific Perplexity MCP server (e.g., `perplexity-ask`, defined in `mcp_config.json`). The command used, like `python mcp_client.py perplexity-ask --query "$(cat intermediate_prompt.txt)"`, passes the entire augmented context (User Query + Video Context from `intermediate_prompt.txt`) as the input query to the client.
    - **Tool Selection (Two Modes):** The client determines which specific Perplexity tool (`perplexity_ask`, `perplexity_reason`, `perplexity_research`) to use based on the active mode:
      - **Default Mode (Claude-Orchestrated):** Without the `--perplexity-mode` flag, the client sends the input query and the list of available Perplexity tools to Claude (`claude-3-7-sonnet-20250219`). Claude acts as an orchestrator, analyzing the query's intent and selecting the most appropriate Perplexity tool to call via MCP.
      - **Rule-Based Mode (`--perplexity-mode`):** When the `--perplexity-mode` flag is present, the client bypasses Claude. It uses its internal `select_perplexity_tool` function, which employs hardcoded logic (analyzing keywords like 'research', 'why', 'explain', and query length) to heuristically choose the best Perplexity tool for the task (e.g., choosing `perplexity_research` for detailed queries, `perplexity_reason` for explanatory ones, and `perplexity_ask` as a default).
    - **MCP Tool Execution:** The client uses the `session.call_tool` function (from the `mcp` library) to execute the selected Perplexity tool on the connected MCP server. The arguments passed to the tool differ based on the mode:
      - In **Default Mode (Claude-Orchestrated)**, Claude processes the full input context (`intermediate_prompt.txt`) and generates specific, refined arguments (`tool_args`) representing the core question distilled from that context. These Claude-generated arguments are then passed to the selected Perplexity tool.
      - In **Rule-Based Mode (`--perplexity-mode`)**, the _entire original input context_ from `intermediate_prompt.txt` is passed directly as the user content within the `messages` structure required by the Perplexity tool.
        Perplexity's server runs the tool with the provided arguments (e.g., performs a web search) and sends the output (high-quality, grounded information from Perplexity's internet search/reasoning capabilities) back to the `mcp_client.py` script via the MCP connection.
    - **Answer Synthesis (`gpt-4o-mini`):** The client extracts the textual results returned by the Perplexity tool. It then constructs a final, detailed prompt for OpenAI's `gpt-4o-mini` model. This prompt contains:
      - The original user query and video context (from `intermediate_prompt.txt`).
      - The relevant internet search results or reasoning output obtained from the Perplexity tool call (labeled as "Additional Information").
    - **Final Output:** `gpt-4o-mini` processes this combined information, synthesizing insights from _both_ the video-specific context and the external information provided by Perplexity, to generate the final, comprehensive answer presented to the user.

This pipeline supports an agentic approach where the AI doesn't just respond passively but actively retrieves relevant information from a structured representation of the video content, leverages Model Context Protocol and uses our MCP Client to query Perplexity's MCP server to add high quality internet search information to the prompt context, before generating its final answer, leading to more accurate and contextually appropriate results.
